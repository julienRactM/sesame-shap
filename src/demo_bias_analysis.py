#!/usr/bin/env python3
"""
Script de d√©monstration du framework d'analyse de biais COMPAS

Ce script montre comment utiliser le framework d'analyse de biais pour √©valuer
l'√©quit√© algorithmique du syst√®me COMPAS et d√©tecter les patterns de discrimination
similaires √† ceux identifi√©s par l'enqu√™te ProPublica.

Usage:
    python demo_bias_analysis.py

Auteur: Claude AI - Assistant Data Engineer
Date: 2025-08-05
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime

# Ajout du r√©pertoire src au path pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from bias_analysis import (
    BiasAnalyzer, BiasAnalysisConfig, run_comprehensive_bias_analysis,
    prepare_protected_attributes
)

def load_latest_compas_data():
    """
    Charge les derni√®res donn√©es COMPAS trait√©es
    
    Returns:
        DataFrame avec les donn√©es COMPAS
    """
    data_dir = "../data/processed"
    
    # Recherche du fichier le plus r√©cent
    compas_files = [f for f in os.listdir(data_dir) if f.startswith('compas_full_') and f.endswith('.csv')]
    
    if not compas_files:
        raise FileNotFoundError("Aucun fichier de donn√©es COMPAS trouv√©")
    
    # Tri par timestamp dans le nom de fichier
    latest_file = sorted(compas_files)[-1]
    file_path = os.path.join(data_dir, latest_file)
    
    print(f"Chargement des donn√©es: {latest_file}")
    df = pd.read_csv(file_path)
    
    return df

def demonstrate_basic_bias_analysis():
    """D√©monstration de l'analyse de biais de base"""
    
    print("\n" + "="*80)
    print("D√âMONSTRATION - ANALYSE DE BIAIS COMPAS")
    print("="*80)
    
    # Chargement des donn√©es
    print("\n1. Chargement des donn√©es COMPAS...")
    df = load_latest_compas_data()
    print(f"   Donn√©es charg√©es: {len(df)} √©chantillons, {len(df.columns)} features")
    
    # Affichage des informations sur les attributs prot√©g√©s
    print("\n2. Analyse des attributs prot√©g√©s...")
    protected_attrs = prepare_protected_attributes(df)
    
    for attr_name, attr_values in protected_attrs.items():
        unique_values, counts = np.unique(attr_values, return_counts=True)
        print(f"   {attr_name}:")
        for val, count in zip(unique_values, counts):
            percentage = (count / len(attr_values)) * 100
            print(f"     - {val}: {count} ({percentage:.1f}%)")
    
    # Configuration de l'analyse
    print("\n3. Configuration de l'analyse de biais...")
    config = BiasAnalysisConfig(
        protected_attributes=list(protected_attrs.keys()),
        fairness_threshold=0.8,  # R√®gle des 80%
        statistical_significance_level=0.05,
        save_visualizations=True,
        results_dir="../data/results/bias_analysis"
    )
    print(f"   Attributs prot√©g√©s: {config.protected_attributes}")
    print(f"   Seuil d'√©quit√©: {config.fairness_threshold}")
    
    # Lancement de l'analyse compl√®te
    print("\n4. Lancement de l'analyse compl√®te de biais...")
    bias_report = run_comprehensive_bias_analysis(
        df=df,
        y_true_col='two_year_recid'
    )
    
    return bias_report

def demonstrate_detailed_metrics_analysis(bias_report):
    """D√©monstration de l'analyse d√©taill√©e des m√©triques"""
    
    print("\n" + "="*80)
    print("ANALYSE D√âTAILL√âE DES M√âTRIQUES D'√âQUIT√â")
    print("="*80)
    
    # R√©sum√© ex√©cutif
    executive_summary = bias_report['executive_summary']
    print(f"\nüö® NIVEAU DE RISQUE: {executive_summary['risk_level']}")
    print(f"üìä R√âSULTATS CL√âS: {executive_summary['key_findings']}")
    print(f"‚ö†Ô∏è  ACTIONS IMM√âDIATES: {executive_summary['recommendation_priority']}")
    
    # Analyse par attribut prot√©g√©
    fairness_metrics = bias_report['fairness_metrics']
    
    for attr_name, attr_metrics in fairness_metrics.items():
        print(f"\n" + "-"*60)
        print(f"ANALYSE POUR: {attr_name.upper()}")
        print("-"*60)
        
        # Parit√© d√©mographique
        if 'demographic_parity' in attr_metrics:
            dp = attr_metrics['demographic_parity']
            print(f"\nüìà PARIT√â D√âMOGRAPHIQUE:")
            print(f"   Impact disparate: {dp['disparate_impact']:.3f}")
            print(f"   R√®gle des 80%: {'‚úÖ RESPECT√âE' if dp['passes_80_rule'] else '‚ùå VIOL√âE'}")
            print(f"   Taux par groupe:")
            for group, rate in dp['group_rates'].items():
                print(f"     - {group}: {rate:.3f}")
        
        # √âgalit√© des chances
        if 'equalized_odds' in attr_metrics:
            eo = attr_metrics['equalized_odds']
            print(f"\n‚öñÔ∏è  √âGALIT√â DES CHANCES:")
            print(f"   Diff√©rence TPR: {eo['tpr_difference']:.3f}")
            print(f"   Diff√©rence FPR: {eo['fpr_difference']:.3f}")
            print(f"   M√©trique globale: {eo['equalized_odds_difference']:.3f}")
            
            for group, metrics in eo['group_metrics'].items():
                print(f"   {group}:")
                print(f"     - TPR: {metrics['tpr']:.3f}, FPR: {metrics['fpr']:.3f}")
        
        # Calibration
        if 'calibration' in attr_metrics:
            cal = attr_metrics['calibration']
            print(f"\nüéØ CALIBRATION:")
            print(f"   Diff√©rence Brier Score: {cal['brier_score_difference']:.3f}")
            print(f"   Diff√©rence ECE: {cal['ece_difference']:.3f}")

def demonstrate_propublica_comparison(bias_report):
    """D√©monstration de la comparaison avec ProPublica"""
    
    print("\n" + "="*80)
    print("COMPARAISON AVEC L'ENQU√äTE PROPUBLICA")
    print("="*80)
    
    propublica_benchmark = bias_report['propublica_benchmark']
    
    print(f"\nüîç COH√âRENCE AVEC PROPUBLICA: {propublica_benchmark['consistency_with_findings'].upper()}")
    
    if 'comparison_with_propublica' in propublica_benchmark:
        comparisons = propublica_benchmark['comparison_with_propublica']
        
        for metric_name, comparison in comparisons.items():
            if isinstance(comparison, dict):
                print(f"\nüìä {metric_name.upper()}:")
                print(f"   Notre analyse: {comparison['our_finding']:.3f}")
                print(f"   R√©f√©rence ProPublica: {comparison['propublica_reference']:.3f}")
                print(f"   Coh√©rence: {'‚úÖ' if comparison['consistent'] else '‚ùå'} {comparison['interpretation']}")
    
    # Contexte historique ProPublica
    print(f"\nüì∞ CONTEXTE PROPUBLICA (2016):")
    print(f"   ‚Ä¢ Les d√©fendeurs afro-am√©ricains √©taient presque 2x plus susceptibles")
    print(f"     d'√™tre √©tiquet√©s √† haut risque que les d√©fendeurs blancs")
    print(f"   ‚Ä¢ Taux de faux positifs: 45% (Afro-Am√©ricains) vs 23% (Blancs)")
    print(f"   ‚Ä¢ COMPAS √©tait plus pr√©cis pour pr√©dire la r√©cidive chez les Blancs")

def demonstrate_mitigation_recommendations(bias_report):
    """D√©monstration des recommandations de mitigation"""
    
    print("\n" + "="*80)
    print("RECOMMANDATIONS DE MITIGATION DES BIAIS")
    print("="*80)
    
    recommendations = bias_report['mitigation_recommendations']
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. üéØ {rec['type'].upper()} - {rec['attribute']}")
        print(f"   S√©v√©rit√©: {rec['severity']}")
        print(f"   Recommandation: {rec['recommendation']}")
        
        if 'technical_approach' in rec and rec['technical_approach']:
            print(f"   Approches techniques:")
            for approach in rec['technical_approach']:
                print(f"     ‚Ä¢ {approach}")

def demonstrate_visualization_generation(bias_report):
    """D√©monstration de la g√©n√©ration de visualisations"""
    
    print("\n" + "="*80)
    print("G√âN√âRATION DES VISUALISATIONS")
    print("="*80)
    
    visualizations = bias_report.get('visualizations', {})
    
    if visualizations:
        print(f"\nüìä {len(visualizations)} visualisations g√©n√©r√©es:")
        
        for viz_name, viz_path in visualizations.items():
            if os.path.exists(viz_path):
                file_size = os.path.getsize(viz_path) / 1024  # KB
                print(f"   ‚úÖ {viz_name}: {viz_path} ({file_size:.1f} KB)")
            else:
                print(f"   ‚ùå {viz_name}: Fichier non trouv√©")
        
        print(f"\nüí° Visualisations disponibles:")
        print(f"   ‚Ä¢ Tableau de bord des m√©triques d'√©quit√©")
        print(f"   ‚Ä¢ Carte de chaleur des biais")
        print(f"   ‚Ä¢ Courbes ROC par groupe d√©mographique")
        print(f"   ‚Ä¢ Graphiques de calibration")
        print(f"   ‚Ä¢ Analyse d'impact disparate")
    else:
        print("\n‚ö†Ô∏è  Aucune visualisation g√©n√©r√©e")

def generate_summary_report(bias_report):
    """G√©n√®re un rapport de synth√®se"""
    
    print("\n" + "="*80)
    print("RAPPORT DE SYNTH√àSE")
    print("="*80)
    
    metadata = bias_report['metadata']
    executive_summary = bias_report['executive_summary']
    
    print(f"\nüìã INFORMATIONS G√âN√âRALES:")
    print(f"   Mod√®le analys√©: {metadata['model_name']}")
    print(f"   Taille de l'√©chantillon: {metadata['sample_size']}")
    print(f"   Attributs prot√©g√©s: {', '.join(metadata['protected_attributes_analyzed'])}")
    print(f"   Date d'analyse: {metadata['analysis_timestamp']}")
    
    print(f"\nüéØ VERDICT FINAL:")
    print(f"   Niveau de risque: {executive_summary['risk_level']}")
    print(f"   Priorit√© d'action: {executive_summary['recommendation_priority']}")
    print(f"   Impact business: {executive_summary['business_impact']}")
    
    # Statistiques de violations
    disparate_impact = bias_report['disparate_impact_analysis']
    total_violations = sum(1 for data in disparate_impact.values() if not data['passes_80_rule'])
    
    print(f"\nüìä STATISTIQUES:")
    print(f"   Violations de la r√®gle des 80%: {total_violations}/{len(disparate_impact)}")
    print(f"   Biais globalement d√©tect√©: {'OUI' if bias_report['bias_patterns']['overall_bias_detected'] else 'NON'}")
    
    # Sauvegarde du rapport de synth√®se
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_path = f"../data/results/bias_analysis/summary_report_{timestamp}.txt"
    
    try:
        os.makedirs("../data/results/bias_analysis", exist_ok=True)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("RAPPORT DE SYNTH√àSE - ANALYSE DE BIAIS COMPAS\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Niveau de risque: {executive_summary['risk_level']}\n")
            f.write(f"Violations d√©tect√©es: {total_violations}\n")
            f.write(f"Recommandations: {len(bias_report['mitigation_recommendations'])}\n\n")
            
            for i, rec in enumerate(bias_report['mitigation_recommendations'], 1):
                f.write(f"{i}. {rec['type']} ({rec['severity']})\n")
                f.write(f"   {rec['recommendation']}\n\n")
        
        print(f"\nüíæ Rapport de synth√®se sauvegard√©: {summary_path}")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la sauvegarde: {e}")

def main():
    """Fonction principale de d√©monstration"""
    
    print("üöÄ D√âMARRAGE DE LA D√âMONSTRATION")
    print("Framework d'Analyse de Biais COMPAS")
    print("D√©tection automatique de discrimination algorithmique")
    
    try:
        # 1. Analyse de biais de base
        bias_report = demonstrate_basic_bias_analysis()
        
        # 2. Analyse d√©taill√©e des m√©triques
        demonstrate_detailed_metrics_analysis(bias_report)
        
        # 3. Comparaison avec ProPublica
        demonstrate_propublica_comparison(bias_report)
        
        # 4. Recommandations de mitigation
        demonstrate_mitigation_recommendations(bias_report)
        
        # 5. G√©n√©ration des visualisations
        demonstrate_visualization_generation(bias_report)
        
        # 6. Rapport de synth√®se
        generate_summary_report(bias_report)
        
        print(f"\n‚úÖ D√âMONSTRATION TERMIN√âE AVEC SUCC√àS")
        print(f"üìÅ R√©sultats disponibles dans: data/results/bias_analysis/")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR LORS DE LA D√âMONSTRATION: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)