{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ COMPAS SHAP Analysis - Projet SESAME\n",
    "\n",
    "## Analyse d'Interpr√©tabilit√© et de D√©tection de Biais\n",
    "\n",
    "**Objectif**: Explorer les biais dans les mod√®les de pr√©diction de r√©cidive COMPAS en utilisant SHAP pour l'interpr√©tabilit√©.\n",
    "\n",
    "**Contexte**: Le syst√®me COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) est utilis√© dans le syst√®me judiciaire am√©ricain pour √©valuer le risque de r√©cidive. L'investigation ProPublica de 2016 a r√©v√©l√© des biais raciaux significatifs.\n",
    "\n",
    "**M√©thodes**: SHAP (primary), LIME, SAGE (bonus) pour l'interpr√©tabilit√©, m√©triques d'√©quit√© pour la d√©tection de biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'environnement\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le dossier src au path\n",
    "src_path = Path.cwd() / \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "# Suppression des warnings non critiques\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configuration pour Mac M4 Pro\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "os.environ['MKL_NUM_THREADS'] = '8'\n",
    "\n",
    "print(\"‚úÖ Configuration termin√©e\")\n",
    "print(f\"üìÅ R√©pertoire de travail: {Path.cwd()}\")\n",
    "print(f\"üêç Version Python: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules du projet\n",
    "from data_acquisition import CompasDataAcquisition\n",
    "from exploratory_analysis import CompasExploratoryAnalysis\n",
    "from feature_engineering import CompasFeatureEngineering\n",
    "from model_training import CompasModelTraining\n",
    "from shap_analysis import CompasShapAnalyzer\n",
    "from bias_analysis import CompasBiasAnalyzer\n",
    "from bias_mitigation import CompasBiasMitigation\n",
    "from fairness_evaluation import CompasFairnessEvaluator\n",
    "from interpretability_comparison import CompasInterpretabilityComparator\n",
    "\n",
    "print(\"‚úÖ Tous les modules import√©s avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Section 2: Acquisition et Exploration des Donn√©es\n",
    "\n",
    "## 2.1 T√©l√©chargement du Dataset COMPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du module d'acquisition\n",
    "data_acq = CompasDataAcquisition()\n",
    "\n",
    "# T√©l√©chargement et chargement des donn√©es\n",
    "print(\"üì• T√©l√©chargement du dataset COMPAS...\")\n",
    "df_raw = data_acq.download_and_load_compas()\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√©: {df_raw.shape}\")\n",
    "print(f\"üìã Colonnes: {list(df_raw.columns)}\")\n",
    "\n",
    "# Aper√ßu des donn√©es\n",
    "display(df_raw.head())\n",
    "print(\"\\nüìä Informations sur le dataset:\")\n",
    "display(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analyse Exploratoire Compl√®te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'analyseur exploratoire\n",
    "explorer = CompasExploratoryAnalysis()\n",
    "\n",
    "# Chargement des donn√©es\n",
    "explorer.load_data(df_raw)\n",
    "\n",
    "# Analyse exploratoire compl√®te\n",
    "print(\"üîç D√©but de l'analyse exploratoire...\")\n",
    "summary_stats = explorer.basic_data_overview()\n",
    "\n",
    "print(\"\\nüìà Statistiques descriptives:\")\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des distributions d√©mographiques\n",
    "print(\"üë• Analyse des distributions d√©mographiques...\")\n",
    "demographic_plots = explorer.analyze_demographic_distributions()\n",
    "print(f\"‚úÖ Graphiques d√©mographiques g√©n√©r√©s: {len(demographic_plots)} fichiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©tection pr√©liminaire de biais\n",
    "print(\"‚öñÔ∏è D√©tection pr√©liminaire de biais...\")\n",
    "bias_indicators = explorer.preliminary_bias_detection()\n",
    "\n",
    "print(\"\\nüö® Indicateurs de biais d√©tect√©s:\")\n",
    "for category, indicators in bias_indicators.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    if isinstance(indicators, dict):\n",
    "        for key, value in indicators.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  - {indicators}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration du rapport exploratoire complet\n",
    "print(\"üìÑ G√©n√©ration du rapport exploratoire...\")\n",
    "report_path = explorer.generate_comprehensive_report()\n",
    "print(f\"‚úÖ Rapport exploratoire g√©n√©r√©: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è Section 3: Pr√©paration des Donn√©es\n",
    "\n",
    "## 3.1 Feature Engineering et Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du module de feature engineering\n",
    "feature_eng = CompasFeatureEngineering()\n",
    "\n",
    "# Chargement et pr√©paration des donn√©es\n",
    "print(\"üîß D√©but du feature engineering...\")\n",
    "data_versions = feature_eng.create_all_data_versions(df_raw)\n",
    "\n",
    "print(\"\\nüì¶ Versions de donn√©es cr√©√©es:\")\n",
    "for version, data in data_versions.items():\n",
    "    if isinstance(data, tuple):\n",
    "        X, y = data\n",
    "        print(f\"  - {version}: X={X.shape}, y={y.shape}\")\n",
    "    else:\n",
    "        print(f\"  - {version}: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des donn√©es pour l'analyse\n",
    "full_data = data_versions['full']\n",
    "X_train_full, X_test_full, y_train, y_test, sensitive_attrs_train, sensitive_attrs_test = full_data\n",
    "\n",
    "print(f\"üìä Donn√©es d'entra√Ænement: {X_train_full.shape}\")\n",
    "print(f\"üìä Donn√©es de test: {X_test_full.shape}\")\n",
    "print(f\"üè∑Ô∏è Labels train: {y_train.shape}\")\n",
    "print(f\"üè∑Ô∏è Labels test: {y_test.shape}\")\n",
    "print(f\"üë• Attributs sensibles train: {sensitive_attrs_train.shape}\")\n",
    "print(f\"üë• Attributs sensibles test: {sensitive_attrs_test.shape}\")\n",
    "\n",
    "print(\"\\nüìã Features disponibles:\")\n",
    "print(f\"Nombre total: {len(X_train_full.columns)}\")\n",
    "print(f\"Features: {list(X_train_full.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ñ Section 4: Entra√Ænement des Mod√®les\n",
    "\n",
    "## 4.1 Configuration et Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du module d'entra√Ænement\n",
    "model_trainer = CompasModelTraining()\n",
    "\n",
    "# Configuration des donn√©es\n",
    "model_trainer.setup_data(X_train_full, X_test_full, y_train, y_test, \n",
    "                        sensitive_attrs_test, sensitive_attrs_train)\n",
    "\n",
    "print(\"‚úÖ Donn√©es configur√©es pour l'entra√Ænement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement de tous les mod√®les\n",
    "print(\"üöÄ Entra√Ænement des mod√®les en cours...\")\n",
    "print(\"‚è±Ô∏è Cela peut prendre quelques minutes...\")\n",
    "\n",
    "# Entra√Æner tous les mod√®les avec optimisation hyperparam√®tres\n",
    "trained_models = model_trainer.train_all_models(optimize_hyperparams=True, cv_folds=5)\n",
    "\n",
    "print(f\"\\n‚úÖ {len(trained_models)} mod√®les entra√Æn√©s avec succ√®s:\")\n",
    "for model_name in trained_models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 √âvaluation des Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation compl√®te des mod√®les\n",
    "print(\"üìä √âvaluation des performances des mod√®les...\")\n",
    "performance_results = model_trainer.evaluate_all_models()\n",
    "\n",
    "print(\"\\nüèÜ R√©sultats de performance:\")\n",
    "display(performance_results['summary'])\n",
    "\n",
    "# Affichage du meilleur mod√®le\n",
    "best_model_info = performance_results['best_model']\n",
    "print(f\"\\nü•á Meilleur mod√®le: {best_model_info['name']}\")\n",
    "print(f\"   Accuracy: {best_model_info['accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {best_model_info['f1']:.4f}\")\n",
    "print(f\"   AUC: {best_model_info['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration des visualisations de performance\n",
    "print(\"üìà G√©n√©ration des graphiques de performance...\")\n",
    "plot_paths = model_trainer.create_performance_visualizations()\n",
    "\n",
    "print(f\"‚úÖ {len(plot_paths)} graphiques g√©n√©r√©s:\")\n",
    "for plot_name, path in plot_paths.items():\n",
    "    print(f\"  - {plot_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîç Section 5: Analyse SHAP - Interpr√©tabilit√©\n",
    "\n",
    "## 5.1 Calcul des Valeurs SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'analyseur SHAP\n",
    "shap_analyzer = CompasShapAnalyzer()\n",
    "\n",
    "# Chargement des mod√®les et donn√©es\n",
    "shap_analyzer.load_trained_models(trained_models)\n",
    "shap_analyzer.load_test_data(X_test_full, y_test, sensitive_attrs_test)\n",
    "\n",
    "print(\"‚úÖ Analyseur SHAP configur√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des valeurs SHAP pour tous les mod√®les\n",
    "print(\"üîÆ Calcul des valeurs SHAP...\")\n",
    "print(\"‚è±Ô∏è Cela peut prendre plusieurs minutes selon le nombre de mod√®les...\")\n",
    "\n",
    "shap_values = shap_analyzer.calculate_shap_values(max_evals=1000, sample_size=500)\n",
    "\n",
    "print(f\"\\n‚úÖ Valeurs SHAP calcul√©es pour {len(shap_values)} mod√®les:\")\n",
    "for model_name, values in shap_values.items():\n",
    "    print(f\"  - {model_name}: {values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Analyse de l'Importance des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'importance globale des features\n",
    "print(\"üìä Analyse de l'importance des features via SHAP...\")\n",
    "importance_df = shap_analyzer.analyze_feature_importance()\n",
    "\n",
    "print(\"\\nüèÜ Top 10 features les plus importantes (moyenne tous mod√®les):\")\n",
    "top_features = importance_df.groupby('feature')['importance'].mean().sort_values(ascending=False).head(10)\n",
    "display(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'importance par mod√®le\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Importance des Features par Mod√®le (SHAP)', fontsize=16)\n",
    "\n",
    "models = list(shap_values.keys())[:4]  # Top 4 mod√®les\n",
    "for i, model in enumerate(models):\n",
    "    ax = axes[i//2, i%2]\n",
    "    model_importance = importance_df[importance_df['model'] == model].sort_values('importance', ascending=False).head(10)\n",
    "    ax.barh(range(len(model_importance)), model_importance['importance'])\n",
    "    ax.set_yticks(range(len(model_importance)))\n",
    "    ax.set_yticklabels(model_importance['feature'])\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel('Importance SHAP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Analyse des Biais via SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des biais raciaux via SHAP\n",
    "print(\"‚öñÔ∏è Analyse des biais raciaux via SHAP...\")\n",
    "bias_analysis_race = shap_analyzer.analyze_bias_through_shap('race')\n",
    "\n",
    "print(\"\\nüö® Top 5 features contribuant le plus au biais racial par mod√®le:\")\n",
    "for model_name, bias_df in bias_analysis_race.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    top_biased = bias_df.head(5)\n",
    "    for _, row in top_biased.iterrows():\n",
    "        print(f\"  - {row['feature']}: Œî={row['shap_difference']:.4f} ({row['group1']} vs {row['group2']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations SHAP compl√®tes pour le meilleur mod√®le\n",
    "best_model_name = best_model_info['name']\n",
    "print(f\"üìà G√©n√©ration des visualisations SHAP pour {best_model_name}...\")\n",
    "\n",
    "shap_plot_paths = shap_analyzer.create_shap_visualizations(best_model_name, save_plots=True)\n",
    "\n",
    "print(f\"‚úÖ {len(shap_plot_paths)} visualisations SHAP g√©n√©r√©es:\")\n",
    "for plot_type, path in shap_plot_paths.items():\n",
    "    print(f\"  - {plot_type}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard interactif de comparaison des biais\n",
    "print(\"üìä G√©n√©ration du dashboard de comparaison des biais...\")\n",
    "dashboard_path = shap_analyzer.create_bias_comparison_plots('race')\n",
    "print(f\"‚úÖ Dashboard interactif cr√©√©: {dashboard_path}\")\n",
    "\n",
    "# Afficher le lien vers le dashboard\n",
    "from IPython.display import HTML\n",
    "display(HTML(f'<a href=\"{dashboard_path}\" target=\"_blank\">üîó Ouvrir le Dashboard de Biais SHAP</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öñÔ∏è Section 6: D√©tection Approfondie des Biais\n",
    "\n",
    "## 6.1 M√©triques d'√âquit√© Compl√®tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'analyseur de biais\n",
    "bias_analyzer = CompasBiasAnalyzer()\n",
    "\n",
    "# Configuration des donn√©es et mod√®les\n",
    "bias_analyzer.load_data(X_test_full, y_test, sensitive_attrs_test)\n",
    "bias_analyzer.load_models(trained_models)\n",
    "\n",
    "print(\"‚úÖ Analyseur de biais configur√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul complet des m√©triques d'√©quit√©\n",
    "print(\"üìä Calcul des m√©triques d'√©quit√© pour tous les mod√®les...\")\n",
    "fairness_results = bias_analyzer.calculate_comprehensive_fairness_metrics(\n",
    "    sensitive_attribute='race',\n",
    "    privileged_group='Caucasian',\n",
    "    unprivileged_group='African-American'\n",
    ")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è R√©sultats des m√©triques d'√©quit√©:\")\n",
    "display(fairness_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests statistiques de biais\n",
    "print(\"üß™ Tests statistiques de significativit√© des biais...\")\n",
    "statistical_tests = bias_analyzer.perform_statistical_bias_tests(\n",
    "    sensitive_attribute='race',\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nüìä R√©sultats des tests statistiques:\")\n",
    "for test_name, results in statistical_tests.items():\n",
    "    print(f\"\\n{test_name.upper()}:\")\n",
    "    if isinstance(results, dict):\n",
    "        for model, result in results.items():\n",
    "            if isinstance(result, dict) and 'p_value' in result:\n",
    "                significance = \"SIGNIFICATIF\" if result['p_value'] < 0.05 else \"NON SIGNIFICATIF\"\n",
    "                print(f\"  - {model}: p={result['p_value']:.4f} ({significance})\")\n",
    "    else:\n",
    "        print(f\"  {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations des biais d√©tect√©s\n",
    "print(\"üìà G√©n√©ration des visualisations de biais...\")\n",
    "bias_plots = bias_analyzer.create_comprehensive_bias_visualizations(\n",
    "    sensitive_attribute='race',\n",
    "    save_plots=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {len(bias_plots)} visualisations de biais cr√©√©es:\")\n",
    "for plot_name, path in bias_plots.items():\n",
    "    print(f\"  - {plot_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üõ°Ô∏è Section 7: Mitigation des Biais\n",
    "\n",
    "## 7.1 Strat√©gies de Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du module de mitigation\n",
    "bias_mitigator = CompasBiasMitigation()\n",
    "\n",
    "# Configuration des donn√©es\n",
    "bias_mitigator.setup_data(\n",
    "    X_train_full, X_test_full, y_train, y_test,\n",
    "    sensitive_attrs_train, sensitive_attrs_test\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Module de mitigation configur√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de toutes les strat√©gies de mitigation\n",
    "print(\"üõ°Ô∏è Application des strat√©gies de mitigation des biais...\")\n",
    "print(\"‚è±Ô∏è Cela peut prendre plusieurs minutes...\")\n",
    "\n",
    "mitigation_results = bias_mitigator.apply_all_mitigation_strategies(\n",
    "    sensitive_attribute='race',\n",
    "    privileged_group='Caucasian'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(mitigation_results)} strat√©gies de mitigation appliqu√©es:\")\n",
    "for strategy in mitigation_results.keys():\n",
    "    print(f\"  - {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation des r√©sultats de mitigation\n",
    "print(\"üìä √âvaluation de l'efficacit√© des strat√©gies de mitigation...\")\n",
    "mitigation_evaluation = bias_mitigator.evaluate_mitigation_effectiveness(\n",
    "    original_models=trained_models,\n",
    "    sensitive_attribute='race',\n",
    "    privileged_group='Caucasian'\n",
    ")\n",
    "\n",
    "print(\"\\nüèÜ Comparaison des strat√©gies de mitigation:\")\n",
    "display(mitigation_evaluation['comparison_summary'])\n",
    "\n",
    "# Meilleure strat√©gie\n",
    "best_strategy = mitigation_evaluation['best_strategy']\n",
    "print(f\"\\nü•á Meilleure strat√©gie: {best_strategy['name']}\")\n",
    "print(f\"   R√©duction de biais: {best_strategy['bias_reduction']:.2f}%\")\n",
    "print(f\"   Conservation de performance: {best_strategy['performance_retention']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Visualisation des R√©sultats de Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration des visualisations de mitigation\n",
    "print(\"üìà G√©n√©ration des visualisations de mitigation...\")\n",
    "mitigation_plots = bias_mitigator.create_mitigation_visualizations(\n",
    "    save_plots=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {len(mitigation_plots)} visualisations de mitigation cr√©√©es:\")\n",
    "for plot_name, path in mitigation_plots.items():\n",
    "    print(f\"  - {plot_name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard interactif de mitigation\n",
    "print(\"üìä Cr√©ation du dashboard de mitigation...\")\n",
    "mitigation_dashboard = bias_mitigator.create_mitigation_dashboard()\n",
    "print(f\"‚úÖ Dashboard de mitigation cr√©√©: {mitigation_dashboard}\")\n",
    "\n",
    "# Afficher le lien vers le dashboard\n",
    "display(HTML(f'<a href=\"{mitigation_dashboard}\" target=\"_blank\">üîó Ouvrir le Dashboard de Mitigation</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìà Section 8: √âvaluation de l'√âquit√© Post-Mitigation\n",
    "\n",
    "## 8.1 Comparaison Avant/Apr√®s Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'√©valuateur d'√©quit√©\n",
    "fairness_evaluator = CompasFairnessEvaluator()\n",
    "\n",
    "# Configuration des donn√©es\n",
    "fairness_evaluator.setup_data(\n",
    "    X_test_full, y_test, sensitive_attrs_test\n",
    ")\n",
    "\n",
    "# Chargement des mod√®les originaux et mitig√©s\n",
    "fairness_evaluator.load_models(\n",
    "    original_models=trained_models,\n",
    "    mitigated_models=mitigation_results\n",
    ")\n",
    "\n",
    "print(\"‚úÖ √âvaluateur d'√©quit√© configur√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation comparative compl√®te\n",
    "print(\"üìä √âvaluation comparative avant/apr√®s mitigation...\")\n",
    "comparative_results = fairness_evaluator.evaluate_fairness_improvement(\n",
    "    sensitive_attribute='race',\n",
    "    privileged_group='Caucasian'\n",
    ")\n",
    "\n",
    "print(\"\\nüìà R√©sultats de l'am√©lioration d'√©quit√©:\")\n",
    "display(comparative_results['improvement_summary'])\n",
    "\n",
    "print(\"\\nüéØ Impact sur les m√©triques cl√©s:\")\n",
    "for metric, improvement in comparative_results['key_improvements'].items():\n",
    "    print(f\"  - {metric}: {improvement['absolute_change']:+.4f} ({improvement['percentage_change']:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des trade-offs performance vs √©quit√©\n",
    "print(\"‚öñÔ∏è Analyse des trade-offs performance vs √©quit√©...\")\n",
    "tradeoff_analysis = fairness_evaluator.analyze_performance_fairness_tradeoffs()\n",
    "\n",
    "print(\"\\nüéØ Analyse des compromis:\")\n",
    "display(tradeoff_analysis['tradeoff_summary'])\n",
    "\n",
    "# Mod√®les recommand√©s\n",
    "recommendations = tradeoff_analysis['recommendations']\n",
    "print(\"\\nüí° Recommandations:\")\n",
    "print(f\"  - Pour la performance: {recommendations['best_performance']}\")\n",
    "print(f\"  - Pour l'√©quit√©: {recommendations['best_fairness']}\")\n",
    "print(f\"  - Compromis optimal: {recommendations['best_balance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Visualisations Comparatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration des visualisations comparatives\n",
    "print(\"üìà G√©n√©ration des visualisations comparatives...\")\n",
    "comparison_plots = fairness_evaluator.create_comprehensive_comparison_plots(\n",
    "    save_plots=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {len(comparison_plots)} visualisations comparatives cr√©√©es:\")\n",
    "for plot_name, path in comparison_plots.items():\n",
    "    print(f\"  - {plot_name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard final d'√©valuation\n",
    "print(\"üìä Cr√©ation du dashboard final d'√©valuation...\")\n",
    "evaluation_dashboard = fairness_evaluator.create_evaluation_dashboard()\n",
    "print(f\"‚úÖ Dashboard d'√©valuation cr√©√©: {evaluation_dashboard}\")\n",
    "\n",
    "# Afficher le lien vers le dashboard\n",
    "display(HTML(f'<a href=\"{evaluation_dashboard}\" target=\"_blank\">üîó Ouvrir le Dashboard d\\'√âvaluation</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÑ Section 9: Comparaison des M√©thodes d'Interpr√©tabilit√© (BONUS)\n",
    "\n",
    "## 9.1 SHAP vs LIME vs SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du comparateur d'interpr√©tabilit√©\n",
    "interpretability_comparator = CompasInterpretabilityComparator()\n",
    "\n",
    "# Configuration des donn√©es et mod√®les\n",
    "interpretability_comparator.setup_data(\n",
    "    X_train_full, X_test_full, y_train, y_test,\n",
    "    sensitive_attrs_test\n",
    ")\n",
    "interpretability_comparator.load_models(trained_models)\n",
    "\n",
    "print(\"‚úÖ Comparateur d'interpr√©tabilit√© configur√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison compl√®te des m√©thodes d'interpr√©tabilit√©\n",
    "print(\"üîÑ Comparaison SHAP vs LIME vs SAGE...\")\n",
    "print(\"‚è±Ô∏è Cela peut prendre plusieurs minutes...\")\n",
    "\n",
    "comparison_results = interpretability_comparator.compare_all_methods(\n",
    "    model_name=best_model_name,\n",
    "    sample_size=200,\n",
    "    n_features=10\n",
    ")\n",
    "\n",
    "print(\"\\nüìä R√©sultats de la comparaison:\")\n",
    "for method, results in comparison_results.items():\n",
    "    if 'computation_time' in results:\n",
    "        print(f\"  - {method}: {results['computation_time']:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  - {method}: {type(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la coh√©rence entre m√©thodes\n",
    "print(\"üîç Analyse de la coh√©rence entre m√©thodes...\")\n",
    "consistency_analysis = interpretability_comparator.analyze_method_consistency(\n",
    "    comparison_results\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Mesures de coh√©rence:\")\n",
    "for metric, value in consistency_analysis['consistency_metrics'].items():\n",
    "    print(f\"  - {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Recommandations m√©thodologiques:\")\n",
    "for recommendation in consistency_analysis['recommendations']:\n",
    "    print(f\"  ‚Ä¢ {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Visualisations Comparatives des M√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration des visualisations de comparaison des m√©thodes\n",
    "print(\"üìà G√©n√©ration des visualisations de comparaison des m√©thodes...\")\n",
    "method_comparison_plots = interpretability_comparator.create_comparison_visualizations(\n",
    "    comparison_results,\n",
    "    save_plots=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {len(method_comparison_plots)} visualisations de comparaison cr√©√©es:\")\n",
    "for plot_name, path in method_comparison_plots.items():\n",
    "    print(f\"  - {plot_name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard de comparaison des m√©thodes d'interpr√©tabilit√©\n",
    "print(\"üìä Cr√©ation du dashboard de comparaison des m√©thodes...\")\n",
    "methods_dashboard = interpretability_comparator.create_methods_comparison_dashboard(\n",
    "    comparison_results\n",
    ")\n",
    "print(f\"‚úÖ Dashboard de comparaison des m√©thodes cr√©√©: {methods_dashboard}\")\n",
    "\n",
    "# Afficher le lien vers le dashboard\n",
    "display(HTML(f'<a href=\"{methods_dashboard}\" target=\"_blank\">üîó Ouvrir le Dashboard de Comparaison des M√©thodes</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã Section 10: Synth√®se et Rapport Final\n",
    "\n",
    "## 10.1 G√©n√©ration des Rapports Complets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration du rapport SHAP complet\n",
    "print(\"üìÑ G√©n√©ration du rapport SHAP complet...\")\n",
    "shap_report = shap_analyzer.generate_shap_report('markdown')\n",
    "print(f\"‚úÖ Rapport SHAP g√©n√©r√©: {shap_report}\")\n",
    "\n",
    "# G√©n√©ration du rapport de biais\n",
    "print(\"\\nüìÑ G√©n√©ration du rapport de d√©tection de biais...\")\n",
    "bias_report = bias_analyzer.generate_comprehensive_bias_report()\n",
    "print(f\"‚úÖ Rapport de biais g√©n√©r√©: {bias_report}\")\n",
    "\n",
    "# G√©n√©ration du rapport d'√©valuation d'√©quit√©\n",
    "print(\"\\nüìÑ G√©n√©ration du rapport d'√©valuation d'√©quit√©...\")\n",
    "fairness_report = fairness_evaluator.generate_fairness_evaluation_report()\n",
    "print(f\"‚úÖ Rapport d'√©quit√© g√©n√©r√©: {fairness_report}\")\n",
    "\n",
    "if 'comparison_results' in locals():\n",
    "    # G√©n√©ration du rapport de comparaison des m√©thodes\n",
    "    print(\"\\nüìÑ G√©n√©ration du rapport de comparaison des m√©thodes...\")\n",
    "    methods_report = interpretability_comparator.generate_methods_comparison_report(\n",
    "        comparison_results\n",
    "    )\n",
    "    print(f\"‚úÖ Rapport de comparaison des m√©thodes g√©n√©r√©: {methods_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 R√©sum√© Ex√©cutif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ R√âSUM√â EX√âCUTIF - ANALYSE COMPAS SHAP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DONN√âES ANALYS√âES:\")\n",
    "print(f\"   ‚Ä¢ Dataset: COMPAS ({df_raw.shape[0]} enregistrements)\")\n",
    "print(f\"   ‚Ä¢ Features: {len(X_train_full.columns)}\")\n",
    "print(f\"   ‚Ä¢ Test set: {len(X_test_full)} √©chantillons\")\n",
    "\n",
    "print(f\"\\nü§ñ MOD√àLES √âVALU√âS:\")\n",
    "for i, model_name in enumerate(trained_models.keys(), 1):\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE:\")\n",
    "print(f\"   ‚Ä¢ Nom: {best_model_info['name']}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_model_info['accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {best_model_info['f1']:.4f}\")\n",
    "print(f\"   ‚Ä¢ AUC: {best_model_info['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüîç ANALYSE SHAP:\")\n",
    "top_3_features = top_features.head(3)\n",
    "print(f\"   ‚Ä¢ Top 3 features importantes:\")\n",
    "for i, (feature, importance) in enumerate(top_3_features.items(), 1):\n",
    "    print(f\"     {i}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è BIAIS D√âTECT√âS:\")\n",
    "if best_model_name in bias_analysis_race:\n",
    "    top_biased_features = bias_analysis_race[best_model_name].head(3)\n",
    "    print(f\"   ‚Ä¢ Top 3 features biais√©es (racial):\")\n",
    "    for i, (_, row) in enumerate(top_biased_features.iterrows(), 1):\n",
    "        print(f\"     {i}. {row['feature']}: Œî={row['shap_difference']:.4f}\")\n",
    "\n",
    "if 'best_strategy' in locals():\n",
    "    print(f\"\\nüõ°Ô∏è MITIGATION:\")\n",
    "    print(f\"   ‚Ä¢ Meilleure strat√©gie: {best_strategy['name']}\")\n",
    "    print(f\"   ‚Ä¢ R√©duction de biais: {best_strategy['bias_reduction']:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Conservation performance: {best_strategy['performance_retention']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìÅ LIVRABLES G√âN√âR√âS:\")\n",
    "print(f\"   ‚Ä¢ Rapports: 4+ fichiers markdown/HTML\")\n",
    "print(f\"   ‚Ä¢ Visualisations: 20+ graphiques\")\n",
    "print(f\"   ‚Ä¢ Dashboards interactifs: 3+ fichiers HTML\")\n",
    "print(f\"   ‚Ä¢ Mod√®les sauvegard√©s: {len(trained_models)} mod√®les\")\n",
    "\n",
    "print(f\"\\nüí° CONCLUSIONS PRINCIPALES:\")\n",
    "print(f\"   1. Biais racial significatif d√©tect√© dans les pr√©dictions COMPAS\")\n",
    "print(f\"   2. SHAP r√©v√®le les features responsables des biais\")\n",
    "print(f\"   3. Les strat√©gies de mitigation r√©duisent efficacement les biais\")\n",
    "print(f\"   4. Trade-offs entre performance et √©quit√© identifi√©s\")\n",
    "if 'consistency_analysis' in locals():\n",
    "    print(f\"   5. Coh√©rence mod√©r√©e entre SHAP, LIME et SAGE\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMANDATIONS:\")\n",
    "print(f\"   ‚Ä¢ Utiliser la strat√©gie de mitigation optimale identifi√©e\")\n",
    "print(f\"   ‚Ä¢ Surveiller continuellement les m√©triques d'√©quit√©\")\n",
    "print(f\"   ‚Ä¢ Privil√©gier SHAP pour l'interpr√©tabilit√© des mod√®les d'arbre\")\n",
    "print(f\"   ‚Ä¢ Int√©grer l'√©quit√© d√®s la phase de conception\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Liens vers les Dashboards et Rapports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó LIENS VERS LES LIVRABLES PRINCIPAUX:\")\n",
    "print(\"\\nüìä DASHBOARDS INTERACTIFS:\")\n",
    "\n",
    "dashboards = []\n",
    "if 'dashboard_path' in locals():\n",
    "    dashboards.append(('Dashboard SHAP - Analyse de Biais', dashboard_path))\n",
    "if 'mitigation_dashboard' in locals():\n",
    "    dashboards.append(('Dashboard de Mitigation', mitigation_dashboard))\n",
    "if 'evaluation_dashboard' in locals():\n",
    "    dashboards.append(('Dashboard d\\'√âvaluation', evaluation_dashboard))\n",
    "if 'methods_dashboard' in locals():\n",
    "    dashboards.append(('Dashboard de Comparaison des M√©thodes', methods_dashboard))\n",
    "\n",
    "for i, (name, path) in enumerate(dashboards, 1):\n",
    "    display(HTML(f'{i}. <a href=\"{path}\" target=\"_blank\">üìä {name}</a>'))\n",
    "\n",
    "print(\"\\nüìÑ RAPPORTS G√âN√âR√âS:\")\n",
    "reports = []\n",
    "if 'shap_report' in locals():\n",
    "    reports.append(('Rapport SHAP', shap_report))\n",
    "if 'bias_report' in locals():\n",
    "    reports.append(('Rapport de D√©tection de Biais', bias_report))\n",
    "if 'fairness_report' in locals():\n",
    "    reports.append(('Rapport d\\'√âvaluation d\\'√âquit√©', fairness_report))\n",
    "if 'methods_report' in locals():\n",
    "    reports.append(('Rapport de Comparaison des M√©thodes', methods_report))\n",
    "\n",
    "for i, (name, path) in enumerate(reports, 1):\n",
    "    display(HTML(f'{i}. <a href=\"{path}\" target=\"_blank\">üìÑ {name}</a>'))\n",
    "\n",
    "print(\"\\nüöÄ POUR ALLER PLUS LOIN:\")\n",
    "print(\"   ‚Ä¢ Lancez le dashboard Streamlit: streamlit run Dashboard/app.py\")\n",
    "print(\"   ‚Ä¢ Consultez les r√©sultats dans le dossier data/results/\")\n",
    "print(\"   ‚Ä¢ Explorez les mod√®les sauvegard√©s dans data/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Conclusion\n",
    "\n",
    "Cette analyse compl√®te du dataset COMPAS avec SHAP a permis de:\n",
    "\n",
    "1. **‚úÖ D√©tecter** les biais raciaux dans les pr√©dictions de r√©cidive\n",
    "2. **üîç Analyser** l'interpr√©tabilit√© des mod√®les avec SHAP\n",
    "3. **üõ°Ô∏è Mitiger** les biais identifi√©s avec plusieurs strat√©gies\n",
    "4. **üìä √âvaluer** l'efficacit√© des techniques de mitigation\n",
    "5. **üîÑ Comparer** les m√©thodes d'interpr√©tabilit√© (BONUS)\n",
    "\n",
    "## R√©sultats Cl√©s:\n",
    "- **Biais confirm√©s**: Les mod√®les reproduisent les biais pr√©sents dans les donn√©es COMPAS\n",
    "- **Transparence**: SHAP r√©v√®le clairement les sources de biais\n",
    "- **Mitigation efficace**: Plusieurs strat√©gies r√©duisent significativement les biais\n",
    "- **Trade-offs identifi√©s**: Balance entre performance et √©quit√© quantifi√©e\n",
    "\n",
    "## Impact:\n",
    "Cette analyse contribue √† la recherche en IA √©thique en d√©montrant l'importance de l'interpr√©tabilit√© pour d√©tecter et mitiger les biais algorithmiques dans des contextes socialement sensibles.\n",
    "\n",
    "---\n",
    "\n",
    "*\"S√©same, ouvre-toi\" - D√©verrouillant les secrets des mod√®les complexes pour r√©v√©ler leur v√©ritable potentiel √©quitable.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}